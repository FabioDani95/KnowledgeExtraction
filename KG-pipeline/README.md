# Knowledge Extraction Pipeline - Modular Architecture

Sistema modulare per l'estrazione semantica di conoscenza da manuali tecnici e la costruzione di knowledge graphs strutturati.

## ğŸ¯ Paradigma Architetturale

Questo sistema implementa un **ecosistema di pipeline verticali** che sostituisce l'approccio monolitico tradizionale. Ogni pipeline Ã¨ specializzata per una categoria documentale omogenea e progettata per popolare un sottoinsieme specifico dello schema JSON di riferimento.

### Motivazione

I manuali tecnici presentano forti variazioni strutturali e linguistiche:
- Alcuni sono tabellari, altri descrittivi
- Diversi livelli di dettaglio tecnico
- Frammentazione delle informazioni

Una pipeline unica produce:
- âŒ Estrazioni parziali o ridondanti
- âŒ Errori nella classificazione delle entitÃ 
- âŒ Relazioni non coerenti
- âŒ DifficoltÃ  di validazione

### Soluzione: Pipeline Verticali

âœ… **Precisione semantica**: Ogni LLM riceve contesto omogeneo e prompt calibrato
âœ… **IncrementalitÃ **: Ogni pipeline puÃ² essere eseguita e validata indipendentemente
âœ… **ScalabilitÃ **: PossibilitÃ  di aggiungere nuove pipeline per altre categorie
âœ… **RiutilizzabilitÃ **: Le pipeline sono combinabili in base al dataset disponibile

---

## ğŸ“‚ Struttura del Progetto

```
KG-pipeline/
â”œâ”€â”€ source/                            # Input documenti organizzati per pipeline
â”‚   â”œâ”€â”€ product_technical/             # Pipeline 1: Dati prodotto/tecnici
â”‚   â”œâ”€â”€ operation_modes/               # Pipeline 2: ModalitÃ  operative
â”‚   â”œâ”€â”€ troubleshooting/               # Pipeline 3: Diagnostica
â”‚   â”œâ”€â”€ repair_structure/              # Pipeline 4: Riparazione e parti
â”‚   â””â”€â”€ testing/                       # Pipeline 5: Test e verifica
â”‚
â”œâ”€â”€ src/                               # Codice sorgente
â”‚   â”œâ”€â”€ core/                          # Moduli riusabili
â”‚   â”‚   â”œâ”€â”€ pdf_reader.py              # Lettura e parsing PDF
â”‚   â”‚   â”œâ”€â”€ text_preprocessor.py       # Normalizzazione testo
â”‚   â”‚   â”œâ”€â”€ schema_validator.py        # Validazione JSON schema
â”‚   â”‚   â””â”€â”€ llm_client.py              # Client OpenAI generico
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/                     # Pipeline verticali
â”‚   â”‚   â”œâ”€â”€ base_pipeline.py           # Classe astratta base
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ product_technical/         # Pipeline 1
â”‚   â”‚   â”‚   â”œâ”€â”€ symbolic_parser.py     # Parser basato su regole
â”‚   â”‚   â”‚   â”œâ”€â”€ neural_extractor.py    # Estrattore LLM-based
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py             # Template prompt specifici
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ operation_modes/           # Pipeline 2
â”‚   â”‚   â”œâ”€â”€ troubleshooting/           # Pipeline 3
â”‚   â”‚   â”œâ”€â”€ repair_structure/          # Pipeline 4
â”‚   â”‚   â””â”€â”€ testing/                   # Pipeline 5
â”‚   â”‚
â”‚   â”œâ”€â”€ merger/                        # Fusione JSON parziali
â”‚   â”‚   â”œâ”€â”€ graph_merger.py            # Merge entities/relations
â”‚   â”‚   â”œâ”€â”€ conflict_resolver.py       # Risoluzione conflitti
â”‚   â”‚   â””â”€â”€ provenance_tracker.py      # Tracciamento provenienza
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrator.py                # Coordinatore pipeline
â”‚   â”œâ”€â”€ utils.py                       # Utilities generiche
â”‚   â””â”€â”€ logger.py                      # Logging configurabile
â”‚
â”œâ”€â”€ output/                            # Risultati estrazione
â”‚   â”œâ”€â”€ partial/                       # JSON parziali per pipeline
â”‚   â”‚   â”œâ”€â”€ product_technical/
â”‚   â”‚   â”œâ”€â”€ operation_modes/
â”‚   â”‚   â”œâ”€â”€ troubleshooting/
â”‚   â”‚   â”œâ”€â”€ repair_structure/
â”‚   â”‚   â””â”€â”€ testing/
â”‚   â”œâ”€â”€ merged/                        # JSON finali fusi
â”‚   â””â”€â”€ logs/                          # Log esecuzione
â”‚
â”œâ”€â”€ schemas/                           # Schema JSON di riferimento
â”‚   â””â”€â”€ neural_extraction.json         # Schema universale
â”‚
â”œâ”€â”€ tests/                             # Test unitari e integrazione
â”‚
â”œâ”€â”€ config.yaml                        # Configurazione globale
â”œâ”€â”€ main.py                            # Entry point
â””â”€â”€ requirements.txt                   # Dipendenze Python
```

---

## ğŸ”§ Le Cinque Pipeline Verticali

### 1ï¸âƒ£ Product & Technical Data
**Input**: Capitoli 1-4 del service manual, specifiche tecniche
**Output**: EntitÃ  fisiche e parametri tecnici
**EntitÃ **: `Product`, `Component`, `ComponentType`, `ParameterSpec`, `Unit`, `RatingPlate`
**Relazioni**: `hasPart`, `hasSpec`, `hasUnit`, `hasRatingPlate`, `instanceOf`
**Strumenti**: OCR + regex + LLM estrattivo per coppie componenti-specifiche

### 2ï¸âƒ£ Operation & Machine Modes
**Input**: User manual, guide operative
**Output**: Sequenze operative e modalitÃ  macchina
**EntitÃ **: `ProcessStep`, `MachineMode`, `MaintenanceTask`, `Tool`, `State`
**Relazioni**: `precedes`, `appliesDuring`, `requiresTool`
**Focus**: Sequenze di azioni e logiche operative

### 3ï¸âƒ£ Troubleshooting & Diagnostics
**Input**: Tabelle errori, codici guasto, regole di riparazione
**Output**: ModalitÃ  di guasto e azioni correttive
**EntitÃ **: `FailureMode`, `RepairAction`, `DiagnosticRule`, `Component`
**Relazioni**: `affects`, `mitigatedBy`, `requiresAction`, `requiresConsumable`
**Focus**: Diagnosi e risoluzione problemi

### 4ï¸âƒ£ Repair & Parts Structure
**Input**: Sezioni disassembly, parts list
**Output**: Albero strutturale del prodotto e procedure di smontaggio
**EntitÃ **: `ComponentType`, `Component`, `Tool`, `ProcessStep`
**Relazioni**: `hasPart`, `belongsTo`, `instanceOf`, `requiresTool`
**Focus**: Gerarchia componenti e procedure riparazione

### 5ï¸âƒ£ Testing & Verification
**Input**: Procedure di test, specifiche di calibrazione
**Output**: Logiche di verifica e soglie accettazione
**EntitÃ **: `TestSpec`, `AnomalyThreshold`, `ParameterSpec`, `Unit`
**Relazioni**: `verifies`, `hasSpec`, `hasUnit`
**Focus**: Collaudo e accettazione finale

---

## ğŸ§© Architettura dei Componenti

### BasePipeline (Template Method Pattern)

Ogni pipeline eredita da una classe astratta base che definisce il workflow standard:

```python
class BasePipeline(ABC):
    def execute(self):
        """Template method: orchestrazione standard"""
        documents = self.load_documents()
        parsed = self.run_symbolic_parsing(documents)
        extracted = self.run_neural_extraction(parsed)
        validated = self.validate_output(extracted)
        return validated

    @abstractmethod
    def run_symbolic_parsing(self, documents):
        """Parsing simbolico specifico della pipeline"""
        pass

    @abstractmethod
    def run_neural_extraction(self, parsed_data):
        """Estrazione neurale specifica della pipeline"""
        pass

    @abstractmethod
    def get_target_entities(self):
        """Ritorna subset di entitÃ  da estrarre"""
        pass
```

### Due Livelli di Estrazione per Pipeline

Ogni pipeline implementa:

1. **Symbolic Parser** (`symbolic_parser.py`)
   - Estrazione basata su pattern, regex, tabelle
   - Identificazione sezioni rilevanti
   - Pre-processing dominio-specifico
   - Output: Struttura intermedia (parsed data)

2. **Neural Extractor** (`neural_extractor.py`)
   - Estrazione LLM-based (OpenAI GPT)
   - Prompt calibrati sul contesto specifico
   - Generazione entitÃ  e relazioni
   - Output: JSON parziale conforme allo schema

### Orchestrator

Coordina l'esecuzione delle pipeline:

```python
class PipelineOrchestrator:
    def run_all(self, parallel=False):
        """Esegue tutte le pipeline"""
        results = []
        for pipeline in self.pipelines:
            result = pipeline.execute()
            results.append(result)

        merged = self.merger.merge(results)
        return merged

    def run_single(self, pipeline_name):
        """Esegue solo una pipeline specifica"""
        pipeline = self.get_pipeline(pipeline_name)
        return pipeline.execute()
```

### Graph Merger

Fonde i JSON parziali in un grafo coerente:

- **Deduplicazione**: Unisce entitÃ  identiche estratte da pipeline diverse
- **Conflict Resolution**: Risolve attributi contrastanti (es. valori diversi per stessa proprietÃ )
- **Provenance Tracking**: Mantiene traccia della pipeline di origine per ogni entitÃ /relazione
- **Schema Validation**: Verifica conformitÃ  del grafo finale allo schema universale

---

## ğŸš€ Installazione e Setup

### 1. Clonare il repository

```bash
git clone <repository-url>
cd KnowledgeExtraction/KG-pipeline
```

### 2. Creare ambiente virtuale

```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# oppure
.venv\Scripts\activate  # Windows
```

### 3. Installare dipendenze

```bash
pip install -r requirements.txt
```

### 4. Configurare API key OpenAI

```bash
cp .env.example .env
# Editare .env e inserire la propria API key
```

### 5. Aggiungere documenti

Copiare i PDF nelle rispettive cartelle in `source/` seguendo le indicazioni in `source/README.md`.

---

## ğŸ’» Utilizzo

### Eseguire tutte le pipeline

```bash
python main.py --mode all
```

### Eseguire una singola pipeline

```bash
# Pipeline 1: Product & Technical Data
python main.py --mode single --pipeline product_technical

# Pipeline 3: Troubleshooting
python main.py --mode single --pipeline troubleshooting
```

### Validare output

```bash
python main.py --mode validate --output output/merged/final_graph.json
```

### Configurazione

Modificare `config.yaml` per:
- Parametri LLM (modello, temperatura, max tokens)
- Filtraggio contenuti
- Impostazioni logging
- Configurazione pipeline-specific

---

## ğŸ“Š Schema JSON Universale

Il file `schemas/neural_extraction.json` definisce:

- **EntitÃ **: 14 tipi (Product, Component, FailureMode, TestSpec, ecc.)
- **Relazioni**: 14 tipi (hasPart, affects, requiresTool, verifies, ecc.)
- **Metadati**: Provenance, confidence scores, source spans
- **Quality metrics**: Validazione, conteggi, warning

Ogni pipeline popola un **sottoinsieme** di questo schema.

---

## ğŸ§ª Approccio Incrementale

### Fase 1: Setup Struttura âœ…
- Struttura cartelle creata
- Schema JSON universale definito
- Configurazione di base

### Fase 2: Pipeline Pilota (Product & Technical Data)
- Implementare `symbolic_parser.py` per estrazione dati tecnici
- Implementare `neural_extractor.py` con prompt calibrato
- Test su documenti reali
- Validazione output

### Fase 3: Graph Merger
- Implementare logica di merge
- Conflict resolution
- Provenance tracking

### Fase 4: Orchestrator
- Coordinamento pipeline
- Logging e monitoraggio
- Report esecuzione

### Fase 5: Altre Pipeline
- Replicare pattern per pipeline 2-5
- Test integrazione end-to-end

---

## ğŸ” Output Generati

### JSON Parziali (output/partial/)

Ogni pipeline genera un JSON parziale:

```json
{
  "document_code": "SERVICE_MANUAL_CH1-4",
  "pipeline": "product_technical",
  "entities": [
    {
      "id": "ent_001",
      "type": "Component",
      "name": "Heating Element",
      "confidence": 0.95
    }
  ],
  "relations": [
    {
      "type": "hasPart",
      "from_ref": "prod_001",
      "to_ref": "ent_001"
    }
  ]
}
```

### JSON Finale (output/merged/)

Il merger produce un grafo unificato con:
- EntitÃ  deduplicate
- Relazioni consolidate
- Provenance per ogni elemento
- Quality metrics globali

---

## ğŸ›¡ï¸ Quality Assurance

- **Schema Validation**: Tutti gli output sono validati contro `neural_extraction.json`
- **Confidence Scores**: Ogni entitÃ /relazione ha un punteggio di confidenza
- **Source Tracing**: Ogni elemento Ã¨ tracciato alla sezione/pagina sorgente
- **Warning System**: Il sistema segnala anomalie, duplicati, inconsistenze

---

## ğŸ“ Logging

I log sono salvati in `output/logs/` con:
- Timestamp esecuzione
- Pipeline eseguita
- Errori e warning
- Statistiche performance (tempo, entitÃ  estratte, token usati)

---

## ğŸ¤ Contribuire

Questo progetto segue un approccio incrementale. Per contribuire:

1. Implementare una pipeline seguendo il pattern di `BasePipeline`
2. Aggiungere test in `tests/`
3. Documentare prompt e strategie nel codice
4. Testare su documenti reali

---

## ğŸ“š Riferimenti

- **Schema JSON**: `schemas/neural_extraction.json`
- **Configurazione**: `config.yaml`
- **Documentazione source**: `source/README.md`

---

## ğŸ“„ Licenza

Progetto di ricerca per estrazione semantica da documentazione tecnica.

## âœ¨ Caratteristiche Tecniche

- **ModularitÃ **: Design SOLID con separation of concerns
- **EstensibilitÃ **: Facile aggiunta di nuove pipeline
- **TracciabilitÃ **: Provenance completa da documento a entitÃ 
- **Robustezza**: Validazione multi-livello e error handling
- **Performance**: PossibilitÃ  di esecuzione parallela delle pipeline

---

**Versione**: 2.0 - Architettura Modulare
**Status**: In sviluppo - Fase 2 (Pipeline Pilota)
